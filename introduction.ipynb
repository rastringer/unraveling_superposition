{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPysOIwmyGWF61BEZJxtE+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/unraveling_superposition/blob/main/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the field of AI Alignment, 'Interpretability' is the study of understanding neural networks; what they learn from training data and how they are forming their predictions.\n",
        "\n",
        "Approaches to interpretability face the curse of dimensionality, as neural networks are a series of compound functions with extremely high-dimensional input spaces. Increases in the number of dimensions mean exponential growth in the volume of input space, making it difficult to understand functions over the vast input space."
      ],
      "metadata": {
        "id": "E1CDm0NB5EEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in interpretability is typically to understand the features a neuron is learning from. In practice, we will see how small a fraction of features we are able to extract in relation to the number of neurons in a network.\n",
        "\n",
        "Neural networks often represent more features than they have dimensions, and mix different, unrelated concepts in single neurons. For example, a neuron in a language model could\n",
        "fire in response to inputs as varied as code in Haskell, Spanish poetry and vehicle descriptions.\n",
        "\n",
        "In this short course, we will examine this fascinating field and look closely at what we can see neural networks are doing, and what we yet cannot.\n"
      ],
      "metadata": {
        "id": "g8uCsxIG5MXt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybWXayM35Q3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}